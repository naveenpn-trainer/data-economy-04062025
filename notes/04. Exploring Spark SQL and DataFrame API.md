# Exploring Spark SQL and DataFrame API

> Spark SQL is one of the module in the spark ecosystem to query and analyze structured and semi-structured 

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd39p7Cc8ZNlgXzqkhemo4SsrpC_xG02Z0atDQUlzsmeZXmDtxCe7LgIc0JkdezEFRysW_eq6X54Uwz7VEjpZ9KU9QT74jjIrKpWh7DwGL3GB3HTPxLQwIN4RZf585goC3bzMEQP10KgKbWGWfSm93Lqy0?key=yGW25KMloT80Lch6YWjT9A)

## Building Spark Application

1. Load the data
2. Process the data
3. Write the result to different destination systems

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdv2jaoYiHRbeQMnfb6YLjiaVkjE6SyxLlZBEWs2S7VZXyt6sMeeqyoV0MnBNUK407a0FyadnNndDVRcmQgfWoNnGq7Hq5kqdsAyN6DPO3gguei-0PsJOQyA9GzX2-F9cXBQY1yQhOU5KeluNaMbnoKdGp8?key=yGW25KMloT80Lch6YWjT9A)

## What is DataFrame

> A DataFrame is a distributed collection of data organized into named columns

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcvvQjfQ2mGYLtFUqZsy3AbhnNBEmsH-_LHmCkwv83dIYhGku6-0FVLZ2Ab7Tg1DQQ7fUhU1dSXeMRVxg6sz00QZFQT3j9lR80SZMUkzNCxwfs0yAGvq1HMs7pNl6z6nJKpMDUTU4SFOyMorMR29taFq4dw?key=yGW25KMloT80Lch6YWjT9A)

* Once you create a dataframe, select, filtering, grouping , sorting, aggregation,s join
* DataFrame = RDD + Schema

1. Hands on - Creating DataFrame
2. Hands on : Process the data and write results



**DataFrame**

```
dfr = spark.read
```

## DataFrameReader Methods

* .csv -> DataFrame
* .json -> DataFrame
* .jdbc -> DataFrame
* .parquet -> DataFrame
* .orc -> DataFrame

## DataFrame Methods

* .printSchema()
* .display()
* .select
* .filter()
* .groupBy()
* .limit
* .columns
* .withColumn
* .drop
* .withColumnRenamed
* .na

## Columns

```
from pyspark.sql.functions import col

df.select(col("name").)
```

Column

* .alias
* .isin
* .between
* .contains
* .isNull
* isNotNull
* .startWith
* .endsWith
* .getItem
* .cast



## pyspark.sql.functions

* col
* lit
* when
* size
* array_contains
* Accessing array with index 
* array_min, array_max, array_distinct
* sort
* split

```
{
	"k1":"v1",
	"k2": {
	
	}
}
```

