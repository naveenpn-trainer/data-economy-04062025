# Exploring Spark Core API's (RDD's)

> RDD's stands for Resilient Distributed Dataset which are the building blocks of an spark application, a fundamental data structured in Apache Spark

**Partitions**

> RDD is a collection of objects that is partitioned and distributed across nodes in a cluster

> A partition is a logical chunk of data (large data)

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfKqQlp0y1Wak5aSLXTXv7k3rl5fx_-YhqJn9vXigCXWwEVwlN9Jc_3mBYotbFT3kcH-X-lxxhdj3yDdoRek6OLPOmEwLLuxMrfJ1BcTxpQSES6X-tIRSJCr1VgYPLmP1KF2S3F_RBkS9-ysJLSSJXQG7vt?key=Dxp7lTxgvspH2ig-I7LuEw)

## RDD Creations

There are two popular ways to create an RDD

1. Create an RDD from collection

   <u>Python</u>

   ```
   L = list(range(1,101))
   numbers_rdd = sc.parallelize(L)
   print(type(numbers_rdd))
   ```

   <u>Scala</u>

   ```
   L = 1 to 101
   val numnbers_rdd = sc.parallelize(L)
   ```

   

2. Create an RDD from external source

   ```
   users_rdd = sc.textFile("c:\users.dat")
   print(type(users_rdd))
   ```

   <u>Scala</u>

   ```
   val users_rdd = sc.textFile("c:\users.dat")
   ```

   

**Important points**

* All the methods to create an RDD is present inside SparkContext (sc)

## RDD Operations

We can perform two types of operations

1. Transformation
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdaTLPuA3Kcg2Bk4awB6GfME5OrwHmQaKlb4xLKMF60gbWuN6NXUkEHbxxoXhs1xp6Dp71vYH-OwAYjAh-DK83ABvBuB5ESwpGDkw7v-X6meF_I8iy-aVh9uZVAov2j967z7NGuI-YkaZnH-1MCLCS-mzDT?key=Dxp7lTxgvspH2ig-I7LuEw)





### Transformation

* Transformation creates a new RDD / DataFrame
* .map(), .filter(), reduceByKey()

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfSCtuOURDdf0K1fvaqUOBv1rQVcnxuN2qMF2exw74N8LP8jxzh35Tkh6VH2D54B1YPXCy5SmfaJsoBWnlEqlAtjRDN9SeWHA-3f-dLo_0sTKOmOG7zpYsqE3lmV2chhoH-Q4klfdR7RpzG8HbvnqVBgi17?key=Dxp7lTxgvspH2ig-I7LuEw)

* When you apply only transformations no jobs will be triggered (because transformation are lazy)

### Actions

* Actions are the operations on RDD / DataFrame to carry out computation

<u>Scala</u>

```scala
val numbers = 1 to 100
val numbersRDD = sc.parallelize(numbers)
val mulBy3RDD = numbersRDD.map(e=>e*3)
val filteredRDD = mulBy3RDD.filter(e=>e%15==0)
filteredRDD.collect()
```

<u>Python</u>

```
L = list(range(1,101))
numbersRDD = sc.parallelize(L)
mulBy3RDD = numbersRDD.map(lambda e:e*3)
filteredRDD = mulBy3RDD.filter(lambda e:e%15==0)
filteredRDD.collect()
```

```python
L = list(range(1,101))
numbersRDD = sc.parallelize(L)
numbersRDD.map(lambda e:e*3).filter(lambda e:e%15==0).collect()
```





