# Getting Started with Apache Spark

> Apache Spark is an **in-memory cluster computing framework**, which is designed to handle a wide range of big data workloads.

* Data Integration and ETL
* Batch Computation
* Stream Processing
* ML Analytics
* Graph Computation

**Important Points**

* Apache Spark is natively written using Scala

## What is PySpark

> PySpark is a Python API for Apache Spark (Distributed Processing Framework)

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-a6DQ2UdOkn50A_DzqonENPIOQgPh0yg6FMZUFloukmRDGh89eEh9jxV3Ph8q4MrA6CljXPyja55oXmhWkmoDnc58m0fn7cz5xl5nx0p6B1NBEjGpkili-6XOT8iJmQWCpAASlCOlXAi3tlSxN3iN0vJO?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcdA_lPXM3GJuWeOl9K7kz_13Wl-MLZxuNVrofpBMt6ecATd6LB7PncLL1iBs5HPiOUGc_s9oLMyfDxw7KgjTa3il7dsXFGjF_g_vR7Vn_lyMM7ccXq5MptfJGMl8ddVjCxuvv4HhypQo9j7b3JKqFWQCim?key=_he-T4Jq934AhrSZa-Be-g)

## Spark Interactive Shell

> It is a command line interface provided by Apache Spark

1. Spark Shell (Scala)

   ```
   spark-shell
   ```

   

2. PySpark Shell (Python)

```
pyspark
```

* Spark uses cluster manager as local

* Every spark application we run we will have Web UI (Track the progress)

* By default two object will be created (sc, spark)

  ```
  >>> type(sc)
  <class 'pyspark.context.SparkContext'>
  >>> type(spark)
  <class 'pyspark.sql.session.SparkSession'>
  ```

## Spark API's for Data Processing

1. Low Level API's (RDD's) <mark>Deprecated</mark>
2. High Level API's (DataFrame, DataSet, SQL)

## Building a Spark Application

1. Load the data
2. Process the data
3. Write the results into destination systems.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd81PL6W-SW8x62XkrfUSxaZVSfDsGra732i-5SL_PLTPIxJEiFProSP6cdC86gr2Gs0y0ehOGsRrCwRF_U417KzdKebr4Xtdpiy2LN2GX9g2vbK6lJe9CyR0dUCm9bJAtCqHOATa_spbVeWGLQZi8iO8c?key=_he-T4Jq934AhrSZa-Be-g)



